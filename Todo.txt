4.6
1. 梯度的含义即该参数对loss值的梯度；
2. 梯度反向传播时的shape即各层的输入输出形状；
3. relu等激活函数不改变形状；
4. 梯度采用链式法则，加号为1，乘法为偏导数；
5. 若要获得W关于loss的偏导数，则依靠dout和x求出；
6. 反之亦然，若求x关于loss的偏导数，则依靠dout和W求出;


4.2
0. trainer => 负责调度其它模块
1. operator => 负责各种算子
2. gradient => 负责求梯度
3. optimizer => 负责根据梯度调整权重
4. layer => 负责建立层
5. model => 负责建立完整模型
